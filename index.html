
<!DOCTYPE html>
<html>

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XB3PR2Y1TQ"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-XB3PR2Y1TQ');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>A2VIS: Amodal-aware Approach to Video Instance Segmentation</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="./assets/css/Highlight-Clean.css">
    <link rel="stylesheet" href="./assets/css/styles.css">

    <!-- <link rel="apple-touch-icon" sizes="180x180" href="./assets/apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" sizes="32x32" href="./favicon-32x32.png"> -->
    <!-- <link rel="icon" type="image/png" sizes="16x16" href="./favicon-16x16.png"> -->
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon.ico">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon.ico">
    <link rel="manifest" href="./site.webmanifest">

    <meta property="og:site_name" content="A2VIS: Amodal-aware Approach to Video Instance Segmentation" />
    <meta property="og:type" content="video.other" />
    <meta property="og:title" content="A2VIS: Amodal-aware Approach to Video Instance Segmentation" />
    <meta property="og:description" content="A2VIS: Amodal-aware Approach to Video Instance Segmentation, 2022." />
    <meta property="og:url" content="" />
    <meta property="og:image" content="" />

    <meta property="article:publisher" content="" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="A2VIS: Amodal-aware Approach to Video Instance Segmentation" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:url" content="" />
    <meta name="twitter:image" content="" />
    <!-- <meta name="twitter:site" content="" /> -->

    <script src="./assets/js/video_comparison.js"></script>
    <script type="module" src="https://unpkg.com/@google/model-viewer@2.0.1/dist/model-viewer.min.js"></script>
</head>

<body>
    <!-- <div class="banner">
      <video class="video lazy"
          poster="https://dreamfusion-cdn.ajayj.com/sept28/banner_1x6_customhue_A.jpg"
          autoplay loop playsinline muted>
        <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/banner_1x6_customhue_A.mp4" type="video/mp4"></source>
      </video>
    </div> -->

    <div class="highlight-clean" style="padding-bottom: 10px;">
        <!-- <div class="container" style="max-width: 768px;"> -->
        <div class="container" style="max-width: 768px;">
            <h1 class="text-center"><b>A2VIS</b>: Amodal-aware Approach to Video Instance Segmentation </h1>
        </div>

        <div class="container" style="max-width: 768px; position: relative; left: 90px;">
            <div class="row authors">
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://trqminh.github.io/">Minh Tran</a></h5>
                    <h6 class="text-center">University of Arkansas</h6>
                </div>
                <div class="col-sm-3">
                    <h5 class="text-center"><a href="https://phamtrongthang123.github.io/">Thang Pham</a></h5>
                    <h6 class="text-center">University of Arkansas</h6>
                </div>
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://uark-aicv.github.io/">Winston Bounsavy</a></h5>
                    <h6 class="text-center">University of Arkansas</h6>
                </div>
            </div>
        </div>

        <div class="container" style="max-width: 768px; position: relative; left: 180px;">
          <div class="row authors">
              <div class="col-sm-3">
                <h5 class="text-center"><a class="text-center" href="https://uark-aicv.github.io/">Tri Nguyen</a></h5>
                <h6 class="text-center">Coupang, Inc.</h6>
              </div>
              <div class="col-sm-3">
                <h5 class="text-center"><a class="text-center" href="https://uark-aicv.github.io/">Ngan Le</a></h5>
                <h6 class="text-center">University of Arkansas</h6>
              </div>
          </div>
        </div>

        <div class="container" style="max-width: 768px;">
          <h4 class="text-center">Preprint</h4>
        </div>

        <div class="buttons" style="margin-bottom: 8px;">
            <a class="btn btn-light" role="button" href="https://arxiv.org/abs/2412.01147">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"></path>
                </svg>
                Paper
            </a>
            <a class="btn btn-light" role="button" href="https://github.com/UARK-AICV/A2VIS">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                  <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                </svg>
                Code
            </a>
            <!-- <a class="btn btn-light" role="button" href="index.html">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" width="24px" height="24px" viewBox="0 0 375 531">
                    <polygon stroke="#000000" points="0.5,0.866 459.5,265.87 0.5,530.874 "/>
                </svg>
                Gallery
            </a> -->
        </div>
    </div>
    <!-- <hr class="divider" /> -->

    <div class="container" style="max-width: 768px;">
        <div class="row captioned_videos">
            <div class="col-md-12">
              
              <!-- <video autoplay loop muted playsinline width="80%" style="display: block; margin: auto"> -->
                <!-- <video autoplay loop muted playsinline width="0%"> -->
                  <!-- <source src="./assets/teaser-vid-v1.mp4" type="video/mp4"> -->
                  <!-- <source src="./assets/teaser-vid-v2.mp4" type="video/mp4"> -->
                <!-- </video> -->
                <img src="./assets/a2vis_teaser.png" style="max-width: 768px;">
                <!-- <video autoplay loop muted playsinline width="20%"> -->
                  <!-- <source src="./assets/" type="video/mp4"> -->
                <!-- </video> -->
                <h6 class="caption" style="text-align: center;"> Comparison between existing VIS and the proposed A2VIS. 
                  By integrating amodal knowledge, A2VIS perceives the complete trajectory and shape of a target. 
                  This contrasts with other VIS methods that do not predict occluded parts, making them inherently susceptible to losing track of the target.
                </h6>
            </div>
        </div>
    </div>

    <!-- <br> -->

    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Abstract</h2>
              <p>
                  <!-- <strong> -->
                    Handling occlusion remains a significant challenge for video instance-level tasks like Multiple Object Tracking (MOT) and Video Instance Segmentation (VIS). In this paper, we propose a novel framework, Amodal-Aware Video Instance Segmentation (A2VIS), which incorporates amodal representations to achieve a reliable and comprehensive understanding of both visible and occluded parts of objects in a video. The key intuition is that awareness of amodal segmentation through spatiotemporal dimension enables a stable stream of object information. In scenarios where objects are partially or completely hidden from view, amodal segmentation offers more consistency and less dramatic changes along the temporal axis compared to visible segmentation. Hence, both amodal and visible information from all clips can be integrated into one global instance prototype. To effectively address the challenge of video amodal segmentation, we introduce the spatiotemporal-prior Amodal Mask Head, which leverages visible information intra clips while extracting amodal characteristics inter clips. Through extensive experiments and ablation studies, we show that A2VIS excels in both MOT and VIS tasks in identifying and tracking object instances with a keen understanding of their full shape.
                  <!-- </strong> -->
              </p>
          </div>
      </div>
  </div>

    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Overview of A2VIS</h2>
                <br>
                <img src="./assets/overall_a2vis/overall_a2vis-1.png" style="max-width: 768px;">
                <br> <br>
                <p>Overall architecture of the proposed A2VIS. IP denotes instance prototypes in this figure. In each clip, 
                  the IP Modelling generates the clip-based IP, which is subsequently updated with the global IP through the IP Update module. 
                  The updated global IP is then used to produce both visible segmentation and amodal segmentation.</p>
                <br>
            </div>
        </div>
    </div>

    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Spatiotemporal-Prior Amodal Mask Head (SAMH)</h2>
                <br>
                <img src="./assets/mask_prediction/mask_prediction-1.png" style="max-width: 768px;">
                <br> <br>
                <p>Network design of Spatiotemporal-prior Amodal Mask Head (SAMH), which takes the frame feature, visible segmentation  
                  and the global instance prototypes as inputs to generate amodal segmentations  
                  and updates the global instance prototypes.</p>
                <br>
            </div>
        </div>
    </div>

    <!-- <hr class="divider" />
    <div class="container" style="max-width: 768px;">
      <div class="row captioned_videos">
          <div class="col-md-12"> -->
            <!-- <h2>SDFusion - Overview</h2> -->
              <!-- Large format devices -->
              <!-- <video class="video lazy d-none d-xs-none d-sm-block" autoplay loop playsinline muted poster="https://dreamfusion-cdn.ajayj.com/sept28/wipe_opposite_6x4_smoothstep.jpg">
                  <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/wipe_opposite_6x4_smoothstep.mp4" type="video/mp4"></source>
              </video> -->
              <!-- Small format devices -->
              <!-- <video class="video lazy d-xs-block d-sm-none" autoplay loop playsinline muted poster="https://dreamfusion-cdn.ajayj.com/sept28/shaded_3x3_smoothstep.jpg">
                  <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/shaded_3x3_smoothstep.mp4" type="video/mp4"></source>
              </video> -->

              <!-- <video class="video lazy"
                poster="./assets/1-teaser-v3-out-1.png"
                autoplay loop playsinline muted>
                <source data-src="./assets/teaser-vid.mp4" type="video/mp4"></source>
              </video> -->

              <!-- <video autoplay loop muted playsinline width="80%" style="display: block; margin: auto"> -->
              <!-- <video autoplay loop muted playsinline width="0%"> -->
                <!-- <source src="./assets/teaser-vid-v1.mp4" type="video/mp4"> -->
                <!-- <source src="./assets/teaser-vid-v2.mp4" type="video/mp4">
              </video> -->

              <!-- <img src="./assets/1-teaser-v3-out-1.png" style="max-width: 768px;"> -->
              <!-- <video autoplay loop muted playsinline width="20%"> -->
                <!-- <source src="./assets/" type="video/mp4"> -->
              <!-- </video> -->
              <!-- <p> <b>SDFusion</b> is a diffusion-based 3D shape generator. It enables various applications.
                (left) SDFusion can generate 3D shapes conditioned on different input modalities, including partial shapes, images, and text. 
                SDFusion can even jointly handle multiple conditioning modalities while controlling the  strength for each of them.
                (right) We showcase an application where we leverage pretrained 2D models to texture 3D shapes generated by SDFusion.</p> -->
              <!-- <h6 class="caption" style="text-align: center;"> SDFusion is a diffusion-based 3D shape generator. It enables various applications.
                (top-left) SDFusion can generate 3D shapes conditioned on different input modalities, including partial shapes, images, and text. 
                (bottom-left) SDFusion can even jointly handle multiple conditioning modalities while controlling the strength for each of them.
                (top-right) We showcase an application where we leverage pretrained 2D models to texture 3D shapes generated by SDFusion. (bottom-right) 
                We use a 3D-printer to print out the generated shapes of SDFusion.
              </h6>
          </div>
      </div>
  </div> -->


    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Qualitative Results</h2>
          </div>
      </div>

      <video autoplay loop muted playsinline width="100%" style="display: block; margin: auto">
        <source src="./assets/video_results_sailvos.mp4" type="video/mp4">
      </video>

      <p> Qualitative results on SAILVOS dataset. </p>

      <video autoplay loop muted playsinline width="100%" style="display: block; margin: auto">
        <source src="./assets/video_results_fishbowl.mp4" type="video/mp4">
      </video>

      <p> Qualitative results on FISHBOWL dataset. </p>

    </div>

    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Quantitative Results</h2>
                <br>
                <img src="./assets/quanti_result.png" style="max-width: 768px;">
                <br> <br>
                <p>Across all backbones and datasets, 
                  A2VIS achieves the highest performance with a significant performance gap with the second best method GenVIS. 
                  Notably, the differences in IDF1 and IDS metrics highlight A2VIS's ability to maintain consistency 
                  and accuracy in object tracking, particularly due to its amodal awareness.</p>
                <br>
            </div>
        </div>
    </div>


    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Citation</h2>
                <!-- comming soon! -->
                <code>
                    @article{tran2024a2vis,<br>
                      &nbsp; title  = {A2VIS: Amodal-aware Approach to Video Instance Segmentation},<br>
                      &nbsp; author={Tran, Minh and Pham, Thang and Bounsavy, Winston and Nguyen, Tri and Le, Ngan},<br>
                      &nbsp; journal={arXiv preprint arXiv:2412.01147},<br>
                      &nbsp; year={2024},<br>
                }</code>
            </div>
        </div>
    </div>

    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
            <h2>Acknowledgement</h2>
          </div>
          <h6> This webpage is borrowed from <a href="https://yccyenchicheng.github.io/SDFusion/">SDFusion</a>. Thanks for their beautiful website! </h6>
      </div>
    </div>

    <script src="https://polyfill.io/v3/polyfill.js?features=IntersectionObserver"></script>
    <script src="/assets/js/yall.js"></script>
    <script>
        yall(
            {
                observeChanges: true
            }
        );
    </script>
    <script src="/assets/js/scripts.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"></script>
    <!-- Import the component -->
</body>

</html>
